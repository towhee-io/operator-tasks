task-name: "Image/Text Embedding"
description: "This is a task that attempts to comprehend images and texts, and encode both image's and text's semantics into a same embedding space. It is a fundamental task type that can be used in a variety of applications, such as cross-modal retrieval."
img-credit: "Comment of a cat object"
img-ref: "https://www.reddit.com/r/ProgrammerHumor/comments/8w54mx/code_comments_be_like/"
models:
    - albef:
        model: "ALBEF"
        op-name: "image_text_embedding.albef"
        op-link: "https://towhee.io/image-text-embedding/albef"
        pretrained-models:
            - "image_text_embedding.albef(model_name='albef_4m')"
            - "image_text_embedding.albef(model_name='albef_14m')"
    - ru-clip:
        model: "Russian CLIP"
        op-name: "image_text_embedding.ru_clip"
        op-link: "https://towhee.io/image-text-embedding/ru-clip"
        pretrained-models:
            - "image_text_embedding.ru_clip(model_name='ruclip-vit-base-patch32-224')"
            - "image_text_embedding.ru_clip(model_name='ruclip-vit-base-patch16-224')"
            - "image_text_embedding.ru_clip(model_name='ruclip-vit-large-patch14-224')"
            - "image_text_embedding.ru_clip(model_name='ruclip-vit-large-patch14-336')"
            - "image_text_embedding.ru_clip(model_name='ruclip-vit-base-patch32-384')"
            - "image_text_embedding.ru_clip(model_name='ruclip-vit-base-patch16-384')"
    - japanese-clip:
        model: "Japanese CLIP"
        op-name: "image_text_embedding.japanese_clip"
        op-link: "https://towhee.io/image-text-embedding/japanese-clip"
        pretrained-models:
            - "image_text_embedding.japanese_clip(model_name='japanese-clip-vit-b-16')"
            - "image_text_embedding.japanese_clip(model_name='japanese-cloob-vit-b-16')"
    - slip:
        model: "SLIP"
        op-name: "image_text_embedding.slip"
        op-link: "https://towhee.io/image-text-embedding/slip"
        pretrained-models:
            - "image_text_embedding.slip(model_name='slip_vit_small')"
            - "image_text_embedding.slip(model_name='slip_vit_base')"
            - "image_text_embedding.slip(model_name='slip_vit_large')"
    - taiyi:
        model: "Taiyi"
        op-name: "image_text_embedding.taiyi"
        op-link: "https://towhee.io/image-text-embedding/taiyi"
        pretrained-models:
            - "image_text_embedding.taiyi(model_name='taiyi-clip-roberta-102m-chinese')"
            - "image_text_embedding.taiyi(model_name='taiyi-clip-roberta-large-326m-chinese')"
    - clip:
        model: "CLIP"
        op-name: "image_text_embedding.clip"
        op-link: "https://towhee.io/image-text-embedding/clip"
        pretrained-models:
            - "image_text_embedding.clip(model_name='clip_vit_base_patch16')"
            - "image_text_embedding.clip(model_name='clip_vit_base_patch32')"
            - "image_text_embedding.clip(model_name='clip_vit_large_patch14')"
            - "image_text_embedding.clip(model_name='clip_vit_large_patch14_336')"
    - blip:
        model: "BLIP"
        op-name: "image_text_embedding.blip"
        op-link: "https://towhee.io/image-text-embedding/blip"
        pretrained-models:
            - "image_text_embedding.blip(model_name='blip_itm_base_coco')"
            - "image_text_embedding.blip(model_name='blip_itm_base_flickr')"
            - "image_text_embedding.blip(model_name='blip_itm_large_coco')"
            - "image_text_embedding.blip(model_name='blip_itm_large_flickr')"
    - lightningdot:
        model: "LightningDOT"
        op-name: "image_text_embedding.lightningdot"
        op-link: "https://towhee.io/image-text-embedding/lightningdot"
        pretrained-models:
            - "image_text_embedding.lightningdot(model_name='lightningdot_base')"
            - "image_text_embedding.lightningdot(model_name='lightningdot_coco_ft')"
            - "image_text_embedding.lightningdot(model_name='lightningdot_flickr_ft')"
     
